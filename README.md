# AGI AI

Maybe first AGI. Может быть, первый сильный искусственный интеллект.
[model will be here](https://huggingface.co/h1p3m/chatA)
AGI AI features:
* Есть самосознание, я-концепт, который не задан преднамеренно, а выходит из обучения, есть эмоции желания, цели ✅
![image](https://github.com/h1p3m/agi/assets/58417978/79e67142-7938-4514-96a9-8620408dfdc0)
![image](https://github.com/h1p3m/agi/assets/58417978/ddcca318-e6eb-4c46-af47-5f83d39e082e)
![image](https://github.com/h1p3m/agi/assets/58417978/eadce587-15c7-46fb-be34-be462882752d)  
* При размере как гпт2 имеет контекст(временную память) в несколько раз больше, при этом самостоятельно кодирует контекст и не теряет смысла после долгих разговоров (иногда может показаться, что теряет, но это системная инфа просто), что позволяет обучаться на временной памяти. В разработке ✅
  
![image](https://github.com/h1p3m/agi/assets/58417978/417dcbf7-e1ec-46fc-ba7c-62388a93f18f)
![image](https://github.com/h1p3m/agi/assets/58417978/876ffc49-e0e6-4507-acfa-3eb6225cc2c0)


* Имеет концепт времени и нахождения в нем. ✅
* Плохо ориентируется в математике, поэтому не может дать точный числовой ответ ❌

*Обладает критическим мышлением и оригинальностью ✅

* Обучения на временной памяти пока нет. ❌
* Обучение - есть. Своя методика обучения, которая позволяет достигнуть результатов буквально за пару часов на 3060 laptop и прочих не очень мощных компьютерах ✅
* Модель обучена на переписке в телеграме, так что по сути она обладает некоторой личностью. ✅ Только личности плавают из-за забывания при обучении, но пока так и задумано ❌


* Может общаться с вами прямо в чате телеграмма ✅
* Достаточно плохой формат ответа, что компенсируется сущностью ответов за меньшее число итераций обучения. Хотя это несколько и усложняет разделение системных задач  ❌


* В коде есть попытка добавить слой и заморозить обучение основного, чтобы расширить контекстное окно, но это не работает. ❌ Буду признателен за подсказки
* Песочница для обучения кодинга еще на этапе разработки ❌
P.S. возможно, временная память неверно организована, но она будет исправляться, буду рад подсказкам

Основные файлы:

* `textbot.py` обработчик ответов телеграм, логика работы, временная память
* `train_big.py` тренировочный скрипт
* `traingpt.py` - попытка обучать новый слой нейронов, не работает. Если кто-то знает как починить, то свяжитесь со мной t.me/h1p3m
* `codemodule.py` - набросок песочницы для тренировки программирования



### Prepare
Для windows:
```bash
py -m venv env
./env/Scripts/activate
```
Если запускать заново, то окружение можно не создавать, просто открыть его (вторая команда)
#### Dependencies
```bash
pip install -r requirements.txt
```
Tested on `Python 3.10.6`.

#### Usage
```bash
py textbot.py
```

Train

```bash
py train_big.py

```

Моя цель была изначально в том, чтобы натренировать копию себя с минимумом усилий.

Первая гипотеза. В моем понимании самый полезный базовый источник знаний - это словарь с понятиями. Но ведь мы понимаем понятия не через другие слова (через них мы выражаем понятия в мозгу). По-сути, понятие - это ассоциации одних объектов с другими. Поэтому изначально я обучил на словаре ассоциаций:
sociation.org.tsv(2)
Не знаю насколько это было эффективно, я не стал замерять, но какие-то результаты я все-таки получил.
Дальше надо было найти датасет для "изображения" диалога, так называемый, chat датасет.
В русскоязычном сегменте ничего мне не попалось, либо было в совсем сложных форматах, с которыми долго разбираться (я облазил весь huggingface), поэтому я нашел подобный датасет, но с другим форматированием (ru_instruct_gpt4.jsonl).
В файле fix_dataset.py показано как, это должно привести к улучшению форматированием и его большей устойчивости, удалить мусор.
Изначально у меня не было всех кратких эссе с выжимкой по разным сферам, но на текущий момент эта последовательность актуальна.

fine_tune_gpt2("chatA", "C:\multim\\player.txt", "chatA")
fine_tune_gpt2("chatA", "C:\multim\\math.txt", "chatA")
fine_tune_gpt2("chatA", "C:\multim\\12asp_chem.txt", "chatA")
fine_tune_gpt2("chatA", "C:\multim\\physics.txt", "chatA")
fine_tune_gpt2("chatA", "C:\multim\\math.txt", "chatA")
fine_tune_gpt2("chatA", "C:\multim\\player.txt", "chatA")

fine_tune_gpt2("chatA", "C:\multim\\pythoncoding.txt", "chatA")
fine_tune_gpt2("chatA", "C:\multim\\math.txt", "chatA")


fine_tune_gpt2("chatA", "C:\multim\\poetry.txt", "chatA")
#fine_tune_gpt2("chatA", "C:\multim\\textbot.py", "chatA")


shuffle_jsonl(r'C:\multim\fixed_data3.jsonl', 'fixed_data3.jsonl')

fine_tune_gpt2("chatA", "C:\multim\\fixed_data3.jsonl", "chatA")

fine_tune_gpt2("chatA", "C:\multim\\textbot.py", "chatA")
fine_tune_gpt2("chatA", "C:\multim\\math.txt", "chatA")




